{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Classification v2\n",
    "## Fine-tuning Yelp-pretrained RoBERTa on Amazon Reviews\n",
    "---\n",
    "**Base model:** RoBERTa fine-tuned on Yelp reviews (Felipe's checkpoint-1000)\n",
    "\n",
    "**Improvements over v1:**\n",
    "1. Stratified train/test split on our Amazon data (80/20)\n",
    "2. Class weights to handle 90/5.6/4.3 imbalance\n",
    "3. Fine-tuning on our actual Amazon data (not just Yelp)\n",
    "4. Early stopping to prevent overfitting\n",
    "5. Proper held-out evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 28,332 rows\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "Positive    25545\n",
      "Negative     1581\n",
      "Neutral      1206\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_cleaned.csv\")\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping check:\n",
      "sentiment  label\n",
      "Negative   0         1581\n",
      "Neutral    1         1206\n",
      "Positive   2        25545\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Map sentiment to numeric labels\n",
    "label_map = {\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
    "id2label = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "df[\"label\"] = df[\"sentiment\"].str.upper().map(label_map)\n",
    "\n",
    "# Verify mapping\n",
    "print(\"Label mapping check:\")\n",
    "print(df.groupby([\"sentiment\", \"label\"]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 22,665 rows\n",
      "Test:  5,667 rows\n",
      "\n",
      "Train distribution:\n",
      "label\n",
      "2    0.9016\n",
      "0    0.0558\n",
      "1    0.0426\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test distribution:\n",
      "label\n",
      "2    0.9017\n",
      "0    0.0558\n",
      "1    0.0425\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test:  {len(test_df):,} rows\")\n",
    "\n",
    "print(f\"\\nTrain distribution:\")\n",
    "print(train_df[\"label\"].value_counts(normalize=True).round(4))\n",
    "print(f\"\\nTest distribution:\")\n",
    "print(test_df[\"label\"].value_counts(normalize=True).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:\n",
      "  NEGATIVE: 5.9723\n",
      "  NEUTRAL: 7.8290\n",
      "  POSITIVE: 0.3697\n",
      "\n",
      "Interpretation: Negative errors cost 6.0x, Neutral errors cost 7.8x, Positive errors cost 0.4x\n"
     ]
    }
   ],
   "source": [
    "# Compute balanced class weights from training data\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=train_df[\"label\"].values\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, w in enumerate(class_weights):\n",
    "    print(f\"  {id2label[i]}: {w:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: Negative errors cost {class_weights[0]:.1f}x, \"\n",
    "      f\"Neutral errors cost {class_weights[1]:.1f}x, \"\n",
    "      f\"Positive errors cost {class_weights[2]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: ./models/yelp_roberta_3class/checkpoint-1000\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer from Felipe's Yelp-pretrained model\n",
    "# UPDATE THIS PATH to point to your Yelp-trained checkpoint\n",
    "YELP_MODEL_PATH = \"./models/yelp_roberta_3class/checkpoint-1000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(YELP_MODEL_PATH)\n",
    "print(f\"Tokenizer loaded from: {YELP_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9ede1c6fd34159964cc846e91954e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22665 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1c626086ef4e3890cccc9b2dda1fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5667 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokenized: 22665 samples\n",
      "Test tokenized:  5667 samples\n",
      "Features: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"reviews.text\", \"label\"]].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"reviews.text\", \"label\"]].reset_index(drop=True))\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"reviews.text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # let DataCollator handle padding (more efficient)\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"reviews.text\"])\n",
    "test_tokenized = test_dataset.map(tokenize_fn, batched=True, remove_columns=[\"reviews.text\"])\n",
    "\n",
    "# Rename label to labels (HuggingFace convention)\n",
    "train_tokenized = train_tokenized.rename_column(\"label\", \"labels\")\n",
    "test_tokenized = test_tokenized.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_tokenized.set_format(\"torch\")\n",
    "test_tokenized.set_format(\"torch\")\n",
    "\n",
    "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
    "print(f\"Test tokenized:  {len(test_tokenized)} samples\")\n",
    "print(f\"Features: {train_tokenized.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Yelp-Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293c73b73817475cb67dbeaa48e169da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: ./models/yelp_roberta_3class/checkpoint-1000\n",
      "Total parameters: 124,647,939\n",
      "Trainable parameters: 124,647,939\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    YELP_MODEL_PATH,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model loaded from: {YELP_MODEL_PATH}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Trainer with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightedTrainer defined.\n",
      "Class weights that will be used: tensor([5.9723, 7.8290, 0.3697], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer that applies class weights to the loss function.\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"WeightedTrainer defined.\")\n",
    "print(f\"Class weights that will be used: {class_weights_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_weighted\": f1_w,\n",
    "        \"precision_weighted\": precision_w,\n",
    "        \"recall_weighted\": recall_w,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured.\n",
      "  Epochs: 5\n",
      "  Batch size: 16\n",
      "  Learning rate: 2e-05\n",
      "  Best model metric: f1_macro\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/amazon_roberta_v2\",\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Evaluation & saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",  # optimize for macro F1 (balances all classes)\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Best model metric: {training_args.metric_for_best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 22,665 samples, evaluating on 5,667 samples\n",
      "Early stopping patience: 3 eval steps without improvement\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3350' max='7085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3350/7085 1:40:19 < 1:51:55, 0.56 it/s, Epoch 2.36/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Precision Weighted</th>\n",
       "      <th>Recall Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.688743</td>\n",
       "      <td>1.091446</td>\n",
       "      <td>0.939474</td>\n",
       "      <td>0.691657</td>\n",
       "      <td>0.728035</td>\n",
       "      <td>0.664503</td>\n",
       "      <td>0.935109</td>\n",
       "      <td>0.932128</td>\n",
       "      <td>0.939474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.080784</td>\n",
       "      <td>0.973947</td>\n",
       "      <td>0.943356</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.737764</td>\n",
       "      <td>0.674208</td>\n",
       "      <td>0.937470</td>\n",
       "      <td>0.934934</td>\n",
       "      <td>0.943356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.895541</td>\n",
       "      <td>0.602914</td>\n",
       "      <td>0.926416</td>\n",
       "      <td>0.703698</td>\n",
       "      <td>0.666805</td>\n",
       "      <td>0.752981</td>\n",
       "      <td>0.932121</td>\n",
       "      <td>0.940109</td>\n",
       "      <td>0.926416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.863935</td>\n",
       "      <td>0.817826</td>\n",
       "      <td>0.946180</td>\n",
       "      <td>0.703505</td>\n",
       "      <td>0.731941</td>\n",
       "      <td>0.702623</td>\n",
       "      <td>0.941794</td>\n",
       "      <td>0.941131</td>\n",
       "      <td>0.946180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>0.666978</td>\n",
       "      <td>0.918299</td>\n",
       "      <td>0.690777</td>\n",
       "      <td>0.709472</td>\n",
       "      <td>0.735049</td>\n",
       "      <td>0.929527</td>\n",
       "      <td>0.950025</td>\n",
       "      <td>0.918299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.658519</td>\n",
       "      <td>0.658106</td>\n",
       "      <td>0.944239</td>\n",
       "      <td>0.739773</td>\n",
       "      <td>0.719491</td>\n",
       "      <td>0.763856</td>\n",
       "      <td>0.945563</td>\n",
       "      <td>0.947589</td>\n",
       "      <td>0.944239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.654268</td>\n",
       "      <td>0.947002</td>\n",
       "      <td>0.943180</td>\n",
       "      <td>0.708552</td>\n",
       "      <td>0.704067</td>\n",
       "      <td>0.733829</td>\n",
       "      <td>0.941581</td>\n",
       "      <td>0.943151</td>\n",
       "      <td>0.943180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.566047</td>\n",
       "      <td>1.006608</td>\n",
       "      <td>0.948474</td>\n",
       "      <td>0.733954</td>\n",
       "      <td>0.736214</td>\n",
       "      <td>0.749626</td>\n",
       "      <td>0.946450</td>\n",
       "      <td>0.947057</td>\n",
       "      <td>0.948474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.562708</td>\n",
       "      <td>1.048969</td>\n",
       "      <td>0.951650</td>\n",
       "      <td>0.749106</td>\n",
       "      <td>0.767315</td>\n",
       "      <td>0.739572</td>\n",
       "      <td>0.948998</td>\n",
       "      <td>0.947510</td>\n",
       "      <td>0.951650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.604025</td>\n",
       "      <td>0.931494</td>\n",
       "      <td>0.949709</td>\n",
       "      <td>0.759234</td>\n",
       "      <td>0.746741</td>\n",
       "      <td>0.774119</td>\n",
       "      <td>0.950033</td>\n",
       "      <td>0.950775</td>\n",
       "      <td>0.949709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.611213</td>\n",
       "      <td>0.787061</td>\n",
       "      <td>0.951650</td>\n",
       "      <td>0.755957</td>\n",
       "      <td>0.762829</td>\n",
       "      <td>0.749758</td>\n",
       "      <td>0.951548</td>\n",
       "      <td>0.951560</td>\n",
       "      <td>0.951650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.774485</td>\n",
       "      <td>0.851629</td>\n",
       "      <td>0.949179</td>\n",
       "      <td>0.760887</td>\n",
       "      <td>0.747065</td>\n",
       "      <td>0.776882</td>\n",
       "      <td>0.949803</td>\n",
       "      <td>0.950816</td>\n",
       "      <td>0.949179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.712898</td>\n",
       "      <td>0.802506</td>\n",
       "      <td>0.954826</td>\n",
       "      <td>0.783127</td>\n",
       "      <td>0.779530</td>\n",
       "      <td>0.787205</td>\n",
       "      <td>0.954756</td>\n",
       "      <td>0.954764</td>\n",
       "      <td>0.954826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.660880</td>\n",
       "      <td>0.750087</td>\n",
       "      <td>0.946180</td>\n",
       "      <td>0.759968</td>\n",
       "      <td>0.759942</td>\n",
       "      <td>0.776049</td>\n",
       "      <td>0.949416</td>\n",
       "      <td>0.954913</td>\n",
       "      <td>0.946180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.469543</td>\n",
       "      <td>0.990966</td>\n",
       "      <td>0.956944</td>\n",
       "      <td>0.789016</td>\n",
       "      <td>0.792399</td>\n",
       "      <td>0.786004</td>\n",
       "      <td>0.956498</td>\n",
       "      <td>0.956102</td>\n",
       "      <td>0.956944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.499039</td>\n",
       "      <td>0.878780</td>\n",
       "      <td>0.954826</td>\n",
       "      <td>0.791177</td>\n",
       "      <td>0.779748</td>\n",
       "      <td>0.803348</td>\n",
       "      <td>0.955657</td>\n",
       "      <td>0.956611</td>\n",
       "      <td>0.954826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08e716d01774b628ea0d3a1f5b613d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b019f7350043cdb3e40e8ee98cf538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5aaa2b6a6e42caab6cd4bec5513d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b518f8fa414643b8fe70050201925d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b211b9981c494dbde2e5d4a6712f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cc7e55df7445438d1e10e921855cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e3502a29c045ccbac2e975f3bdaa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ee96f38a224795b0c0fccebc1ae170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f11c58b08954b30ac3be7d9d8052a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a32dfdb35341d7adbafda15d401b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5ac390153347fd87ced2f1db7d2226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be1e4f6489a49cb95e7e466b57daa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44c0639fb6440bdb34e8ddf5321d25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49d665704c540178fdc44862ae48f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef32bb31d3b74504a51c146798ea36db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106051202b22430fa47679028ae9687d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights_tensor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(train_tokenized):,} samples, evaluating on {len(test_tokenized):,} samples\")\n",
    "print(f\"Early stopping patience: 3 eval steps without improvement\")\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(f\"Training completed in {train_result.metrics['train_runtime']:.0f} seconds\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Total steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_tokenized)\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Classification report\n",
    "labels = [id2label[i] for i in range(3)]\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT (Amazon Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix (counts)\")\n",
    "axes[0].set_ylabel(\"True Label\")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "\n",
    "# Normalized by true label (recall per class)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2%\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
    "axes[1].set_title(\"Confusion Matrix (normalized by true label)\")\n",
    "axes[1].set_ylabel(\"True Label\")\n",
    "axes[1].set_xlabel(\"Predicted Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare v1 vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 results (from Felipe's notebook - on full dataset, no train/test split)\n",
    "v1_results = {\n",
    "    \"Accuracy\": 0.8723,\n",
    "    \"F1 Macro\": 0.6347,\n",
    "    \"Negative F1\": 0.6985,\n",
    "    \"Neutral F1\": 0.2689,\n",
    "    \"Positive F1\": 0.9367\n",
    "}\n",
    "\n",
    "# v2 results (from this notebook - on held-out test set)\n",
    "report = classification_report(y_true, y_pred, target_names=labels, digits=4, output_dict=True)\n",
    "v2_results = {\n",
    "    \"Accuracy\": report[\"accuracy\"],\n",
    "    \"F1 Macro\": report[\"macro avg\"][\"f1-score\"],\n",
    "    \"Negative F1\": report[\"NEGATIVE\"][\"f1-score\"],\n",
    "    \"Neutral F1\": report[\"NEUTRAL\"][\"f1-score\"],\n",
    "    \"Positive F1\": report[\"POSITIVE\"][\"f1-score\"]\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame({\"v1 (Yelp only)\": v1_results, \"v2 (+ Amazon fine-tune)\": v2_results})\n",
    "comparison[\"Change\"] = comparison[\"v2 (+ Amazon fine-tune)\"] - comparison[\"v1 (Yelp only)\"]\n",
    "print(\"Model Comparison:\")\n",
    "print(\"Note: v1 was evaluated on full dataset (no split), v2 on held-out test set\")\n",
    "print(comparison.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at misclassified reviews\n",
    "test_df_eval = test_df.reset_index(drop=True).copy()\n",
    "test_df_eval[\"predicted\"] = [id2label[p] for p in y_pred]\n",
    "test_df_eval[\"true\"] = [id2label[t] for t in y_true]\n",
    "test_df_eval[\"correct\"] = test_df_eval[\"predicted\"] == test_df_eval[\"true\"]\n",
    "\n",
    "errors = test_df_eval[~test_df_eval[\"correct\"]]\n",
    "print(f\"Total errors: {len(errors)} / {len(test_df_eval)} ({len(errors)/len(test_df_eval)*100:.1f}%)\")\n",
    "print(f\"\\nError breakdown:\")\n",
    "print(errors.groupby([\"true\", \"predicted\"]).size().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample misclassified reviews by type\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ERRORS: Positive predicted as Negative\")\n",
    "print(\"=\" * 60)\n",
    "subset = errors[(errors[\"true\"] == \"POSITIVE\") & (errors[\"predicted\"] == \"NEGATIVE\")]\n",
    "for _, row in subset.head(5).iterrows():\n",
    "    print(f\"  Rating: {row['reviews.rating']} | Text: {str(row['reviews.text'])[:150]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ERRORS: Negative predicted as Positive\")\n",
    "print(\"=\" * 60)\n",
    "subset = errors[(errors[\"true\"] == \"NEGATIVE\") & (errors[\"predicted\"] == \"POSITIVE\")]\n",
    "for _, row in subset.head(5).iterrows():\n",
    "    print(f\"  Rating: {row['reviews.rating']} | Text: {str(row['reviews.text'])[:150]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ERRORS: Neutral misclassified\")\n",
    "print(\"=\" * 60)\n",
    "subset = errors[errors[\"true\"] == \"NEUTRAL\"]\n",
    "for _, row in subset.head(5).iterrows():\n",
    "    print(f\"  Rating: {row['reviews.rating']} | Predicted: {row['predicted']} | Text: {str(row['reviews.text'])[:150]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "FINAL_MODEL_PATH = \"./models/amazon_roberta_v2_final\"\n",
    "\n",
    "trainer.save_model(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"Model saved to: {FINAL_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with the saved model\n",
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=FINAL_MODEL_PATH,\n",
    "    tokenizer=FINAL_MODEL_PATH,\n",
    "    device=device,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "test_reviews = [\n",
    "    \"This product is terrible, waste of money.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Absolutely love it! Best purchase I've made.\",\n",
    "    \"good\",\n",
    "    \"Batteries died after one week. Very disappointed.\",\n",
    "    \"Works as expected for the price.\",\n",
    "]\n",
    "\n",
    "print(\"Quick model test:\")\n",
    "print(\"-\" * 60)\n",
    "for review in test_reviews:\n",
    "    result = clf(review)\n",
    "    top = max(result, key=lambda x: x[\"score\"])\n",
    "    print(f\"  [{top['label']:>8s} {top['score']:.2%}] {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generate Predictions for Full Dataset\n",
    "\n",
    "Run the model on all reviews and save for use in clustering and summarization notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on full dataset\n",
    "all_texts = df[\"reviews.text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "print(f\"Running predictions on {len(all_texts):,} reviews...\")\n",
    "all_preds = clf(all_texts, batch_size=64, truncation=True, max_length=256)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and confidence scores\n",
    "df[\"predicted_label\"] = [max(item, key=lambda x: x[\"score\"])[\"label\"] for item in all_preds]\n",
    "df[\"predicted_score\"] = [max(item, key=lambda x: x[\"score\"])[\"score\"] for item in all_preds]\n",
    "\n",
    "def get_score(item, label):\n",
    "    for i in item:\n",
    "        if i[\"label\"] == label:\n",
    "            return i[\"score\"]\n",
    "    return None\n",
    "\n",
    "df[\"score_negative\"] = [get_score(item, \"NEGATIVE\") for item in all_preds]\n",
    "df[\"score_neutral\"] = [get_score(item, \"NEUTRAL\") for item in all_preds]\n",
    "df[\"score_positive\"] = [get_score(item, \"POSITIVE\") for item in all_preds]\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"data_with_predictions_v2.csv\", index=False)\n",
    "print(f\"Saved predictions to data_with_predictions_v2.csv\")\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(df[\"predicted_label\"].value_counts())\n",
    "\n",
    "print(f\"\\nTrue vs Predicted:\")\n",
    "true_col = df[\"sentiment\"].str.upper()\n",
    "pred_col = df[\"predicted_label\"]\n",
    "print(classification_report(true_col, pred_col, target_names=labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "**Model:** RoBERTa-base → fine-tuned on Yelp 3-class → fine-tuned on Amazon reviews with class weights\n",
    "\n",
    "**Key improvements over v1:**\n",
    "- Class weights penalize errors on minority classes (Negative ~6x, Neutral ~8x vs Positive)\n",
    "- Trained on actual Amazon data, not just Yelp transfer\n",
    "- Proper stratified 80/20 train/test split\n",
    "- Early stopping prevents overfitting (patience=3 on macro F1)\n",
    "- Optimized for macro F1 instead of accuracy (better for imbalanced data)\n",
    "\n",
    "**Files produced:**\n",
    "- `./models/amazon_roberta_v2_final/` — saved model for deployment\n",
    "- `data_with_predictions_v2.csv` — full dataset with predicted labels and confidence scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
