{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Classification v2\n",
    "## Fine-tuning Yelp-pretrained RoBERTa on Amazon Reviews\n",
    "---\n",
    "**Base model:** RoBERTa fine-tuned on Yelp reviews (Felipe's checkpoint-1000)\n",
    "\n",
    "**Improvements over v1:**\n",
    "1. Stratified train/test split on our Amazon data (80/20)\n",
    "2. Class weights to handle 90/5.6/4.3 imbalance\n",
    "3. Fine-tuning on our actual Amazon data (not just Yelp)\n",
    "4. Early stopping to prevent overfitting\n",
    "5. Proper held-out evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data_cleaned.csv\")\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map sentiment to numeric labels\n",
    "label_map = {\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n",
    "id2label = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "df[\"label\"] = df[\"sentiment\"].str.upper().map(label_map)\n",
    "\n",
    "# Verify mapping\n",
    "print(\"Label mapping check:\")\n",
    "print(df.groupby([\"sentiment\", \"label\"]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test:  {len(test_df):,} rows\")\n",
    "\n",
    "print(f\"\\nTrain distribution:\")\n",
    "print(train_df[\"label\"].value_counts(normalize=True).round(4))\n",
    "print(f\"\\nTest distribution:\")\n",
    "print(test_df[\"label\"].value_counts(normalize=True).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute balanced class weights from training data\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=train_df[\"label\"].values\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, w in enumerate(class_weights):\n",
    "    print(f\"  {id2label[i]}: {w:.4f}\")\n",
    "\n",
    "print(f\"\\nInterpretation: Negative errors cost {class_weights[0]:.1f}x, \"\n",
    "      f\"Neutral errors cost {class_weights[1]:.1f}x, \"\n",
    "      f\"Positive errors cost {class_weights[2]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer from Felipe's Yelp-pretrained model\n",
    "# UPDATE THIS PATH to point to your Yelp-trained checkpoint\n",
    "YELP_MODEL_PATH = \"./models/yelp_roberta_3class/checkpoint-1000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(YELP_MODEL_PATH)\n",
    "print(f\"Tokenizer loaded from: {YELP_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"reviews.text\", \"label\"]].reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"reviews.text\", \"label\"]].reset_index(drop=True))\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"reviews.text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # let DataCollator handle padding (more efficient)\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "train_tokenized = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"reviews.text\"])\n",
    "test_tokenized = test_dataset.map(tokenize_fn, batched=True, remove_columns=[\"reviews.text\"])\n",
    "\n",
    "# Rename label to labels (HuggingFace convention)\n",
    "train_tokenized = train_tokenized.rename_column(\"label\", \"labels\")\n",
    "test_tokenized = test_tokenized.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_tokenized.set_format(\"torch\")\n",
    "test_tokenized.set_format(\"torch\")\n",
    "\n",
    "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
    "print(f\"Test tokenized:  {len(test_tokenized)} samples\")\n",
    "print(f\"Features: {train_tokenized.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Yelp-Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    YELP_MODEL_PATH,\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model loaded from: {YELP_MODEL_PATH}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Trainer with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer that applies class weights to the loss function.\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"WeightedTrainer defined.\")\n",
    "print(f\"Class weights that will be used: {class_weights_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_weighted\": f1_w,\n",
    "        \"precision_weighted\": precision_w,\n",
    "        \"recall_weighted\": recall_w,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/amazon_roberta_v2\",\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Evaluation & saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",  # optimize for macro F1 (balances all classes)\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Performance\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Best model metric: {training_args.metric_for_best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    class_weights=class_weights_tensor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(train_tokenized):,} samples, evaluating on {len(test_tokenized):,} samples\")\n",
    "print(f\"Early stopping patience: 3 eval steps without improvement\")\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(f\"Training completed in {train_result.metrics['train_runtime']:.0f} seconds\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "print(f\"Total steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "predictions = trainer.predict(test_tokenized)\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Classification report\n",
    "labels = [id2label[i] for i in range(3)]\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT (Amazon Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true, y_pred, target_names=labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "axes[0].set_title(\"Confusion Matrix (counts)\")\n",
    "axes[0].set_ylabel(\"True Label\")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "\n",
    "# Normalized by true label (recall per class)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2%\", cmap=\"Blues\",\n",
    "            xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
    "axes[1].set_title(\"Confusion Matrix (normalized by true label)\")\n",
    "axes[1].set_ylabel(\"True Label\")\n",
    "axes[1].set_xlabel(\"Predicted Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare v1 vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 results (from Felipe's notebook - on full dataset, no train/test split)\n",
    "v1_results = {\n",
    "    \"Accuracy\": 0.8723,\n",
    "    \"F1 Macro\": 0.6347,\n",
    "    \"Negative F1\": 0.6985,\n",
    "    \"Neutral F1\": 0.2689,\n",
    "    \"Positive F1\": 0.9367\n",
    "}\n",
    "\n",
    "# v2 results (from this notebook - on held-out test set)\n",
    "report = classification_report(y_true, y_pred, target_names=labels, digits=4, output_dict=True)\n",
    "v2_results = {\n",
    "    \"Accuracy\": report[\"accuracy\"],\n",
    "    \"F1 Macro\": report[\"macro avg\"][\"f1-score\"],\n",
    "    \"Negative F1\": report[\"NEGATIVE\"][\"f1-score\"],\n",
    "    \"Neutral F1\": report[\"NEUTRAL\"][\"f1-score\"],\n",
    "    \"Positive F1\": report[\"POSITIVE\"][\"f1-score\"]\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame({\"v1 (Yelp only)\": v1_results, \"v2 (+ Amazon fine-tune)\": v2_results})\n",
    "comparison[\"Change\"] = comparison[\"v2 (+ Amazon fine-tune)\"] - comparison[\"v1 (Yelp only)\"]\n",
    "print(\"Model Comparison:\")\n",
    "print(\"Note: v1 was evaluated on full dataset (no split), v2 on held-out test set\")\n",
    "print(comparison.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at misclassified reviews\n",
    "test_df_eval = test_df.reset_index(drop=True).copy()\n",
    "test_df_eval[\"predicted\"] = [id2label[p] for p in y_pred]\n",
    "test_df_eval[\"true\"] = [id2label[t] for t in y_true]\n",
    "test_df_eval[\"correct\"] = test_df_eval[\"predicted\"] == test_df_eval[\"true\"]\n",
    "\n",
    "errors = test_df_eval[~test_df_eval[\"correct\"]]\n",
    "print(f\"Total errors: {len(errors)} / {len(test_df_eval)} ({len(errors)/len(test_df_eval)*100:.1f}%)\")\n",
    "print(f\"\\nError breakdown:\")\n",
    "print(errors.groupby([\"true\", \"predicted\"]).size().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample misclassified reviews by type\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ERRORS: Positive predicted as Negative\")\n",
    "print(\"=\" * 60)\n",
    "subset = errors[(errors[\"true\"] == \"POSITIVE\") & (errors[\"predicted\"] == \"NEGATIVE\")]\n",
    "for _, row in subset.head(5).iterrows():\n",
    "    print(f\"  Rating: {row['reviews.rating']} | Text: {str(row['reviews.text'])[:150]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ERRORS: Negative predicted as Positive\")\n",
    "print(\"=\" * 60)\n",
    "subset = errors[(errors[\"true\"] == \"NEGATIVE\") & (errors[\"predicted\"] == \"POSITIVE\")]\n",
    "for _, row in subset.head(5).iterrows():\n",
    "    print(f\"  Rating: {row['reviews.rating']} | Text: {str(row['reviews.text'])[:150]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE ERRORS: Neutral misclassified\")\n",
    "print(\"=\" * 60)\n",
    "subset = errors[errors[\"true\"] == \"NEUTRAL\"]\n",
    "for _, row in subset.head(5).iterrows():\n",
    "    print(f\"  Rating: {row['reviews.rating']} | Predicted: {row['predicted']} | Text: {str(row['reviews.text'])[:150]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "FINAL_MODEL_PATH = \"./models/amazon_roberta_v2_final\"\n",
    "\n",
    "trainer.save_model(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"Model saved to: {FINAL_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with the saved model\n",
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=FINAL_MODEL_PATH,\n",
    "    tokenizer=FINAL_MODEL_PATH,\n",
    "    device=device,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "test_reviews = [\n",
    "    \"This product is terrible, waste of money.\",\n",
    "    \"It's okay, nothing special but does the job.\",\n",
    "    \"Absolutely love it! Best purchase I've made.\",\n",
    "    \"good\",\n",
    "    \"Batteries died after one week. Very disappointed.\",\n",
    "    \"Works as expected for the price.\",\n",
    "]\n",
    "\n",
    "print(\"Quick model test:\")\n",
    "print(\"-\" * 60)\n",
    "for review in test_reviews:\n",
    "    result = clf(review)\n",
    "    top = max(result, key=lambda x: x[\"score\"])\n",
    "    print(f\"  [{top['label']:>8s} {top['score']:.2%}] {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generate Predictions for Full Dataset\n",
    "\n",
    "Run the model on all reviews and save for use in clustering and summarization notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on full dataset\n",
    "all_texts = df[\"reviews.text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "print(f\"Running predictions on {len(all_texts):,} reviews...\")\n",
    "all_preds = clf(all_texts, batch_size=64, truncation=True, max_length=256)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and confidence scores\n",
    "df[\"predicted_label\"] = [max(item, key=lambda x: x[\"score\"])[\"label\"] for item in all_preds]\n",
    "df[\"predicted_score\"] = [max(item, key=lambda x: x[\"score\"])[\"score\"] for item in all_preds]\n",
    "\n",
    "def get_score(item, label):\n",
    "    for i in item:\n",
    "        if i[\"label\"] == label:\n",
    "            return i[\"score\"]\n",
    "    return None\n",
    "\n",
    "df[\"score_negative\"] = [get_score(item, \"NEGATIVE\") for item in all_preds]\n",
    "df[\"score_neutral\"] = [get_score(item, \"NEUTRAL\") for item in all_preds]\n",
    "df[\"score_positive\"] = [get_score(item, \"POSITIVE\") for item in all_preds]\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"data_with_predictions_v2.csv\", index=False)\n",
    "print(f\"Saved predictions to data_with_predictions_v2.csv\")\n",
    "\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(df[\"predicted_label\"].value_counts())\n",
    "\n",
    "print(f\"\\nTrue vs Predicted:\")\n",
    "true_col = df[\"sentiment\"].str.upper()\n",
    "pred_col = df[\"predicted_label\"]\n",
    "print(classification_report(true_col, pred_col, target_names=labels, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "**Model:** RoBERTa-base → fine-tuned on Yelp 3-class → fine-tuned on Amazon reviews with class weights\n",
    "\n",
    "**Key improvements over v1:**\n",
    "- Class weights penalize errors on minority classes (Negative ~6x, Neutral ~8x vs Positive)\n",
    "- Trained on actual Amazon data, not just Yelp transfer\n",
    "- Proper stratified 80/20 train/test split\n",
    "- Early stopping prevents overfitting (patience=3 on macro F1)\n",
    "- Optimized for macro F1 instead of accuracy (better for imbalanced data)\n",
    "\n",
    "**Files produced:**\n",
    "- `./models/amazon_roberta_v2_final/` — saved model for deployment\n",
    "- `data_with_predictions_v2.csv` — full dataset with predicted labels and confidence scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
